{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Variable\n",
    "\n",
    "A random variable, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables. They are as follows:\n",
    "1. Discrete Random Variable\n",
    "2. Continuous Random Variable\n",
    "\n",
    "### 1.1 Discrete Random Variable\n",
    "\n",
    "If a random variable can take one value from a finite set of values then such type of random variable is said to be __Discrete Random Variable.__\n",
    "Example: In case of dice experiment (Rolling a dice), the random variable say \"X\" is a discrete random variable as it can have only values among 6 values of dice faces which are {1,2,3,4,5,6}\n",
    "\n",
    "The _probability distribution_ of a discrete random variable is a list of probabilities associated with each of its possible values. It is also sometimes called the probability function or the probability mass function.\n",
    "\n",
    "Suppose a random variable X may take k different values, with the probability that $X = x_i$ defined to be $P(X = x_i) = p_i$. The probabilities $p_i$ must satisfy the following:\n",
    "\n",
    "1. 0 < $p_i$ < 1 for each i\n",
    "2. $p_1$ + $p_2$ + ... + $p_k$ = 1.\n",
    "\n",
    "<img src=\"img/drv.png\" />\n",
    "\n",
    "Suppose a variable X can take the values 1, 2, 3, or 4. \n",
    "The probabilities associated with each outcome are described by the following table:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Outcome</b></td>\n",
    "        <td>1</td>\n",
    "        <td>2</td>\n",
    "        <td>3</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Probability</strong></td>\n",
    "        <td>0.1</td>\n",
    "        <td>0.3</td>\n",
    "        <td>0.4</td>\n",
    "        <td>0.2</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The probability that X is equal to 2 or 3 is the sum of the two probabilities: $P(X = 2 or X = 3) = P(X = 2) + P(X = 3) = 0.3 + 0.4 = 0.7$. Similarly, the probability that Xis greater than 1 is equal to $1 - P(X = 1) = 1 - 0.1 = 0.9$, by the complement rule.\n",
    "This distribution may also be described by the probability histogram shown above.\n",
    "\n",
    "\n",
    "All random variables (discrete and continuous) have a ___cumulative distribution function___. It is a function giving the probability that the random variable X is less than or equal to x, for every value x. For a discrete random variable, the cumulative distribution function is found by summing up the probabilities.\n",
    "\n",
    "<img src=\"img/cdf.png\">\n",
    "\n",
    "The cumulative distribution function for the above probability distribution is calculated as follows: \n",
    "* The probability that X is less than or equal to 1 is 0.1, \n",
    "* The probability that X is less than or equal to 2 is 0.1+0.3 = 0.4, \n",
    "* The probability that X is less than or equal to 3 is 0.1+0.3+0.4 = 0.8, and \n",
    "* The probability that X is less than or equal to 4 is 0.1+0.3+0.4+0.2 = 1.\n",
    "\n",
    "The probability histogram for the cumulative distribution of this random variable is shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Continuous Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a random variable can take infinite number of real values then such type of random variable is ___continuous random variable___. Continuous random variables are usually measurements. For example: height, weight of a person, amount of sugar in a cup of tea, the time required to run a mile.\n",
    "\n",
    "A continuous random variable is not defined at specific values. Instead, it is defined over an interval of values, and is represented by the ___area under a curve___. The probability of observing any single value is equal to 0, since the number of values which may be assumed by the random variable is infinite.\n",
    "\n",
    "Suppose a random variable X may take all values over an interval of real numbers. Then the probability that X is in the set of outcomes $A, P(A)$, is defined to be the area above A and under the curve. The curve which represents a function $p(x)$, must satisfy the following conditions:\n",
    "1. The curve has no negative values i.e $( p(x) \\geqslant 0$ for all $x )$\n",
    "2. The total area under the curve must be equal to 1.\n",
    "\n",
    "A curve meeting these requirements is called as ___density curve___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Population and Sample\n",
    "\n",
    "The main difference between a population and sample has to do with how observations are assigned to the dataset.\n",
    "\n",
    "* A __population__ includes all of the elements from a set of data.\n",
    "* A __sample__ consists one or more observation drawn from the population. Also, it can be thought as the subset of population.\n",
    "\n",
    "Depending on the sampling method, a sample can have fewer observations that the population, the same number of observations, or more observations. More than one sample can be derived from the same population.\n",
    "\n",
    "Other differences have to do with nomenclature, notation and computations. For example:\n",
    "* A measurable characterstic of a population, such as mean or standard deviation, is called a ___parameter___.; but a measurable characterstic of a sample is called a statistic.\n",
    "* The mean of a population is denoted by the symbol $\\mu$; whereas the mean of the sample is denoted by symbol $\\overline{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian / Normal Distribution\n",
    "\n",
    "In probability theory, a normal (or Gaussian or Gaus or Laplace-Gauss) distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "The parameter $\\mu$ is the population mean or expectation of the distribution; $\\sigma$ is its standard deviation.\n",
    "\n",
    "Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distribution are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly normal.\n",
    "\n",
    "Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of normal deviates is a normal deviate.  Many results and methods (such as propagation of uncertainity and least squares parameters fitting) can be derived analytically in explicit form when the relevant variables are normally distributed. For more details, [Click Here](https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "\n",
    "For the normal distribution, the values less than one standard deviation away from the mean account for 68.268% of the set; while two standard deviations from the mean account for 95.45%; and three standard deviations accounts for 99.73% which is shown in the figure below:\n",
    "\n",
    "<img src=\"img/nd.png\">\n",
    "\n",
    "Area under the curve represents the probability of an event happening between two points.\n",
    "Let two points on the distribution be, a and b then, to find the probability we use the following equation:\n",
    "\n",
    "$$\n",
    "P(a \\leqslant X \\leqslant b) = \\int_{a}^{b} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}  dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cumulative Density Function of Gaussian Distribution\n",
    "\n",
    "The cumulative distribution of a normal distributed population with various population mean and variance is shown below:\n",
    "\n",
    "<img src=\"img/cdf_nd.png\" width=500 height=200>\n",
    "\n",
    "For more details, [Click Here](https://en.wikipedia.org/wiki/Cumulative_distribution_function)\n",
    "\n",
    "__Observation:__\n",
    "1. As $\\sigma^2$ or variance of the distribution increases, the distance of the S curve (CDF) from the mean increases.\n",
    "2. The CDF curve always starts from 0 and goes upto maximum value i.e 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measures of Skewness and Kurtosis\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><strong>Skewness and Kurtosis</strong></td>\n",
    "        <td>\n",
    "            <p>A fundamental task in many statistical analyses is to characterize the location and variability of a dataset. A further characterization of the data includes skewness and kurtosis.</p>\n",
    "            <p>Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.</p>\n",
    "            <p>Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with hight kurtosis tend to have heavy tails or outliers or high pick value. Data sets with low kurtosis tend to have light tails or lack of outliers. A uniform distribution would be the extreme case.</p>\n",
    "            <p>The histogram is an effective graphical technique for showing both the skewness and kurtosis of data set.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The following diagram shows the types of skewness:\n",
    "<img src=\"img/skewness.png\" width=500 height=300>\n",
    "\n",
    "Consider the two distribution in the figure above. Within each graph, the values on the right side of the distribution taper differently from the values on the left side. These tapering sides are called _tails_, and they provide visual means to determine which of the two kinds of skewness a distribution has:\n",
    "\n",
    "1. _negative skew_ : The left tail is longer; the mass of the distribution is concentrated on the right of the figure. The distribution is said to be left-skewed, left-tailed, or skewed to the left, despite the fact that the curve itself appears to be kewed or leaning to the right; left instead refers to the left tail being drawn out and, often, the mean being skewed to the left of a typical center of the data. A left-skewed distribution usually appears as a right-learning curve.\n",
    "\n",
    "2. _positive skew_ : The right tail is longer; the mass of the distribution is concentrated on the left of the figure. The distribution is said to be right-skewed, right-tailed, or skewed to the right, despite the fact that the curve itself apperas to be skewed or leaning to the left; right instead refers to the right tail being drawn out and, often, the mean being skewed to the right of a typical center of the data. A right-skewed distribution usually appears as a left-leaning curve. For more details, [Click Here](https://en.wikipedia.org/wiki/Skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Central Limit Theorem (CLT)\n",
    "\n",
    "In the study of probability, the central limit theorem states that the distribution of sample means approximates a normal distribution, as the sample size becomes larges, assuming that all samples are identical in size, and regardless of the population distribution shape.\n",
    "\n",
    "$$\n",
    "\\overline{x_i} => N(\\mu, \\frac {\\sigma^2} {n})\n",
    "$$\n",
    "where, $\\overline{x_i}$ is sample mean\n",
    "\n",
    "Also, CLT is a statistical theory stating that given a sufficiently large sample size from a population with a finite level of variance, the mean of all samples from the same population will be approximately equal to the mean of the population. Furthermore, all the samples will follow an approximate normal distribution pattern, with all variances being approximately equal to the variance of the population divided by each sample's size.\n",
    "\n",
    "As a general rule, sample sizes equal to or greater than 30 are deemed sufficient for the CLT to hold, meaning that the distribution of the sample means is fairly normally distributed. Therefore, the more samples one takes, the more the graphed result take the shape of a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quantile-Quantile (Q-Q) Plot\n",
    "\n",
    "The Q-Q plot is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a normal or exponential. For example, if we run a statistical analysis that assumes our dependent variable is Normally distributed, we can use a Normal Q-Q plot to check that assumption. It's just a visual check, not an air-tight proof, so it is somewhat subjective. But it allows us to see at-a-glance if our assumption is plausible and if not, how the assumption is violated and what data points contribute to the violation.\n",
    "\n",
    "A Q-Q plot is a scatter plot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that's roughtly straight. Here’s an example of a Normal Q-Q plot when both sets of quantiles truly come from Normal distributions.\n",
    "\n",
    "<img src=\"img/qq.jpeg\" alt=\"Normal Q-Q plot\">\n",
    "\n",
    "Now what are “quantiles”? These are often referred to as “percentiles”. These are points in your data below which a certain proportion of your data fall. For example, imagine the classic bell-curve standard Normal distribution with a mean of 0. The 0.5 quantile, or 50th percentile, is 0. Half the data lie below 0. That’s the peak of the hump in the curve. The 0.95 quantile, or 95th percentile, is about 1.64. 95 percent of the data lie below 1.64.\n",
    "\n",
    "[Click Here](https://data.library.virginia.edu/understanding-q-q-plots/), for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
